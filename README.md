# Black-Box Adversarial Attack on MNIST (Transfer-Based)

This project demonstrates how to perform a **black-box adversarial attack** using **transferability** between models. The attacker does **not have access to the target modelâ€™s architecture or gradients**, but can train a local substitute model and craft adversarial inputs using it.

## ğŸ§  What It Demonstrates

- A **target model** is trained on MNIST but treated as a black-box: we never access its weights or gradients.
- A **surrogate model** is trained by the attacker with the same data distribution.
- The attacker generates adversarial examples on the surrogate using **FGSM**.
- These examples are used to attack the target.  
â†’ If the target misclassifies, the transfer attack was successful.

## ğŸ§ª Workflow

| Step | Description |
|------|-------------|
| 1ï¸âƒ£ | Train a target model on MNIST (to be attacked later) |
| 2ï¸âƒ£ | Train a surrogate model (different architecture) |
| 3ï¸âƒ£ | Generate adversarial examples on the surrogate using FGSM |
| 4ï¸âƒ£ | Feed those examples to the target model and measure performance drop |

## âš”ï¸ Attack Technique

**Fast Gradient Sign Method (FGSM)**  
Adds a small perturbation in the direction of the gradient to cause misclassification.  
On the surrogate model:

```python
perturbed = image + epsilon * sign(âˆ‡_image Loss(model(image), label))

These perturbed images are then used against the target model â€” even though no gradients or internal access is available for it.

## Results

| Model          | Clean Accuracy | Under Attack (Îµ=0.2) |
| -------------- | -------------- | -------------------- |
| **Target (MLP)**   | ~97%           | ~XX% (drops!)        |
| **Surrogate**      | ~95%           | â€“                    |

**Transfer success rate:** â‰ˆâ€¯XX%  
*(Percentage of adversarial examples crafted on the surrogate that fooled the target model.)*

---

## Why This Is Realistic

- **No whiteâ€‘box access:** Attacker never sees the victimâ€™s gradients or internal structure.  
- **Surrogate-based crafting:** Adversarial examples are generated locally on a standâ€‘in model.  
- **Effective transfer:** Perturbed inputs still fool the unseen target with high success.  

_Common realâ€‘world uses:_  
- Facial recognition evasion  
- Security bypass of ML models in production  
- Adversarial robustness testing  

---

## Files

- `main.ipynb` â€” Full pipeline with training, FGSM attack, and transfer evaluation  
- `model_target.pth` â€” Saved weights simulating the victim (target) model  
- `.venv/` â€” Local Python environment (excluded from repository)  

---

## Author

**Emanuele Rossi**  
AI Red Teaming portfolio  
_Project 3 of the Red Team series: blackâ€‘box adversarial ML_  
